{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import logging\n",
    "import streamlit as st\n",
    "\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "\n",
    "\n",
    "DB_PATH = \"chroma_db\"\n",
    "PERSIS_DIR = \"./chroma_langchain_db\"\n",
    "load_dotenv() #loads all env vars\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_documents(release_data: list[str], embeddings: OpenAIEmbeddings) -> Chroma:\n",
    "    \"\"\"\n",
    "    Save documents to a Chroma database with embeddings.\n",
    "\n",
    "    Args:\n",
    "        release_data (List[str]): List of text data to be saved.\n",
    "        embeddings (OpenAIEmbeddings): Embedding model to use for creating document embeddings.\n",
    "\n",
    "    Returns:\n",
    "        Chroma: The Chroma database object with the saved documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "    docs = []\n",
    "    for data in release_data:   \n",
    "        docs.extend(text_splitter.create_documents([data]))\n",
    "    \n",
    "    db = Chroma.from_documents(docs, embeddings, persist_directory=PERSIS_DIR)\n",
    "    db.persist()\n",
    "    return db\n",
    "\n",
    "\n",
    "def load_text_files(directory_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Load all text files from a specified directory using LangChain's DirectoryLoader.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path to the directory containing text files.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text data loaded from the files.\n",
    "    \"\"\"\n",
    "    loader = DirectoryLoader(directory_path, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    documents = loader.load()\n",
    "    return [doc.page_content for doc in documents]\n",
    "\n",
    "\n",
    "def retrive_docs(chroma_db: Chroma, llm: ChatOpenAI, query: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents from a Chroma database using a language model.\n",
    "\n",
    "    Args:\n",
    "        chroma_db (Chroma): The Chroma database object.\n",
    "        llm (ChatOpenAI): The language model to use for retrieval.\n",
    "        query (str): The query string to search for.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of unique documents retrieved based on the query.\n",
    "    \"\"\"\n",
    "    retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "        retriever=chroma_db.as_retriever(), llm=llm\n",
    "    )\n",
    "\n",
    "    unique_docs = retriever_from_llm.invoke(query)\n",
    "    return unique_docs\n",
    "\n",
    "\n",
    "def get_response(llm: ChatOpenAI, docs: list[str], query: str) -> str:\n",
    "    \"\"\"\n",
    "    Get a response from the language model based on the provided documents and query.\n",
    "\n",
    "    Args:\n",
    "        llm (ChatOpenAI): The language model to use for generating the response.\n",
    "        docs (List[str]): The list of documents to use as context.\n",
    "        query (str): The query string to ask the language model.\n",
    "\n",
    "    Returns:\n",
    "        str: The response generated by the language model.\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You have to answer question based on context given:\\n\\n{context}\"),\n",
    "            (\"user\", \"Question:\\n\\n{query}\")\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "    llm_response = chain.invoke({\"context\": docs, \"query\": query})\n",
    "    return llm_response\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "        Main function to load or save documents, and retrieve and get responses based on a query.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    if os.path.exists(PERSIS_DIR):\n",
    "        chroma_db = Chroma(persist_directory=PERSIS_DIR, embedding_function=embeddings)\n",
    "    else:\n",
    "        release_data_list = load_text_files('data')\n",
    "        chroma_db = save_documents(release_data_list, embeddings)\n",
    "\n",
    "    query = \"What are the all different partnerships and collboration made by T-Systems. List them and give some info on them\"\n",
    "    \n",
    "    # Initialize the language model with the specified temperature and API key\n",
    "    llm = ChatOpenAI(temperature=0.6, api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    docs = retrive_docs(chroma_db, llm, query)\n",
    "    \n",
    "    if query: \n",
    "        llm_response = get_response(llm, docs, query)\n",
    "        if llm_response:\n",
    "            pprint(llm_response)\n",
    "        else:\n",
    "            logging.info(\"No Response recived from the LLM !\")\n",
    "    else:\n",
    "        logging.info(\"Please provide the search query !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T-Systems has established partnerships and collaborations with various '\n",
      " 'entities to enhance its services and offerings. Here are the key '\n",
      " 'partnerships and collaborations:\\n'\n",
      " '\\n'\n",
      " '1. **Partnership with MAN Energy Solutions**: T-Systems collaborates closely '\n",
      " 'with MAN Energy Solutions for the migration of its IT infrastructure to the '\n",
      " 'public or hybrid cloud. This collaboration aims to achieve flexibility and '\n",
      " 'scalability by integrating the experience and scalable capacity of '\n",
      " 'T-Systems.\\n'\n",
      " '\\n'\n",
      " '2. **Hyper-scalers (Google, AWS, Azure)**: T-Systems partners with major '\n",
      " 'hyper-scalers like Google, Amazon Web Services (AWS), and Microsoft Azure to '\n",
      " 'provide tailored cloud solutions for different workloads. By leveraging '\n",
      " 'these partnerships, T-Systems can offer private cloud, public cloud, and '\n",
      " 'hybrid cloud solutions to its customers.\\n'\n",
      " '\\n'\n",
      " '3. **Open Telekom Cloud and Private Future Cloud Infrastructure**: T-Systems '\n",
      " 'offers its own cloud solutions, including the Open Telekom Cloud and Private '\n",
      " 'Future Cloud Infrastructure based on VMware. These solutions provide '\n",
      " 'customers with a range of cloud services tailored to their specific needs.\\n'\n",
      " '\\n'\n",
      " '4. **Catena-X Automotive Network**: T-Systems is a co-founder of the '\n",
      " 'Catena-X initiative, specifically the \"Catena-X Automotive Network.\" This '\n",
      " 'initiative focuses on bringing transparency to supply chains in the '\n",
      " 'automotive industry and establishing a uniform standard for data and '\n",
      " 'information flows throughout the automotive value chain. T-Systems plays a '\n",
      " 'key role in enhancing connectivity, automation, and resilience in the '\n",
      " 'automotive sector through this collaboration.\\n'\n",
      " '\\n'\n",
      " \"These partnerships and collaborations highlight T-Systems' commitment to \"\n",
      " 'offering innovative and secure cloud computing solutions, driving digital '\n",
      " 'transformation in various industries, and supporting clients in developing '\n",
      " 'their digital strategies.')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsystems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
