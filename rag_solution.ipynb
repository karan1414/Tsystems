{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import logging\n",
    "import streamlit as st\n",
    "\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "\n",
    "\n",
    "DB_PATH = \"chroma_db\"\n",
    "PERSIS_DIR = \"./chroma_langchain_db\"\n",
    "load_dotenv() #loads all env vars\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_documents(release_data: list[str], embeddings: OpenAIEmbeddings) -> Chroma:\n",
    "    \"\"\"\n",
    "    Save documents to a Chroma database with embeddings.\n",
    "\n",
    "    Args:\n",
    "        release_data (List[str]): List of text data to be saved.\n",
    "        embeddings (OpenAIEmbeddings): Embedding model to use for creating document embeddings.\n",
    "\n",
    "    Returns:\n",
    "        Chroma: The Chroma database object with the saved documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "    docs = []\n",
    "    for data in release_data:   \n",
    "        docs.extend(text_splitter.create_documents([data]))\n",
    "    \n",
    "    db = Chroma.from_documents(docs, embeddings, persist_directory=PERSIS_DIR)\n",
    "    db.persist()\n",
    "    return db\n",
    "\n",
    "\n",
    "def load_text_files(directory_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Load all text files from a specified directory using LangChain's DirectoryLoader.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path to the directory containing text files.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text data loaded from the files.\n",
    "    \"\"\"\n",
    "    loader = DirectoryLoader(directory_path, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "    documents = loader.load()\n",
    "    return [doc.page_content for doc in documents]\n",
    "\n",
    "\n",
    "def retrive_docs(chroma_db: Chroma, llm: ChatOpenAI, query: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents from a Chroma database using a language model.\n",
    "\n",
    "    Args:\n",
    "        chroma_db (Chroma): The Chroma database object.\n",
    "        llm (ChatOpenAI): The language model to use for retrieval.\n",
    "        query (str): The query string to search for.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of unique documents retrieved based on the query.\n",
    "    \"\"\"\n",
    "    retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "        retriever=chroma_db.as_retriever(), llm=llm\n",
    "    )\n",
    "\n",
    "    unique_docs = retriever_from_llm.invoke(query)\n",
    "    return unique_docs\n",
    "\n",
    "\n",
    "def get_response(llm: ChatOpenAI, docs: list[str], query: str) -> str:\n",
    "    \"\"\"\n",
    "    Get a response from the language model based on the provided documents and query.\n",
    "\n",
    "    Args:\n",
    "        llm (ChatOpenAI): The language model to use for generating the response.\n",
    "        docs (List[str]): The list of documents to use as context.\n",
    "        query (str): The query string to ask the language model.\n",
    "\n",
    "    Returns:\n",
    "        str: The response generated by the language model.\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You have to answer question based on context given:\\n\\n{context}\"),\n",
    "            (\"user\", \"Question:\\n\\n{query}\")\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "    llm_response = chain.invoke({\"context\": docs, \"query\": query})\n",
    "    return llm_response\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "        Main function to load or save documents, and retrieve and get responses based on a query.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    if os.path.exists(PERSIS_DIR):\n",
    "        chroma_db = Chroma(persist_directory=PERSIS_DIR, embedding_function=embeddings)\n",
    "    else:\n",
    "        release_data_list = load_text_files('data')\n",
    "        chroma_db = save_documents(release_data_list, embeddings)\n",
    "\n",
    "    query = \"What are the all different partnerships and collboration made by T-Systems. List them and give some info on them\"\n",
    "    \n",
    "    # Initialize the language model with the specified temperature and API key\n",
    "    llm = ChatOpenAI(temperature=0.6, api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    docs = retrive_docs(chroma_db, llm, query)\n",
    "    \n",
    "    if query: \n",
    "        llm_response = get_response(llm, docs, query)\n",
    "        if llm_response:\n",
    "            pprint(llm_response)\n",
    "        else:\n",
    "            logging.info(\"No Response recived from the LLM !\")\n",
    "    else:\n",
    "        logging.info(\"Please provide the search query !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karan\\AppData\\Local\\Temp\\ipykernel_30052\\3209599535.py:88: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  chroma_db = Chroma(persist_directory=PERSIS_DIR, embedding_function=embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T-Systems has formed various partnerships and collaborations to enhance its '\n",
      " 'offerings in the digital space. Here are the key partnerships mentioned in '\n",
      " 'the text:\\n'\n",
      " '\\n'\n",
      " '1. **Partner T-Systems**: T-Systems collaborates with its partner, '\n",
      " 'T-Systems, to fully migrate its IT infrastructure to the public or hybrid '\n",
      " 'cloud. This partnership allows for flexibility and scalability required for '\n",
      " 'their operations. Close collaboration based on agile methods is emphasized '\n",
      " 'to enable integration.\\n'\n",
      " '\\n'\n",
      " '2. **Google, AWS, and Azure hyper-scalers**: T-Systems utilizes the services '\n",
      " 'of Google, AWS, and Azure hyper-scalers to tailor cloud solutions for '\n",
      " 'different workloads. This partnership allows for offering private cloud, '\n",
      " 'public cloud, and hybrid cloud solutions to customers.\\n'\n",
      " '\\n'\n",
      " '3. **VMware**: T-Systems leverages VMware for its Private Future Cloud '\n",
      " 'Infrastructure. This partnership enables T-Systems to provide cloud '\n",
      " 'solutions based on VMware technology.\\n'\n",
      " '\\n'\n",
      " \"These partnerships and collaborations showcase T-Systems' commitment to \"\n",
      " 'providing a wide range of cloud solutions and services tailored to the needs '\n",
      " 'of its customers.')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsystems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
